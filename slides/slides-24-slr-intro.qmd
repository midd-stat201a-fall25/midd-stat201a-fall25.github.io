---
title: "Introduction to Simple Linear Regression"
date: "April 21, 2025"
title-slide-attributes:
    data-background-image: "figs/bikeshare-plots.png"
    data-background-size: contain
    data-background-opacity: "0.2"
format: 
  revealjs:
    theme: custom.scss
    transition: none
    incremental: true
    scrollable: true
editor: visual
editor_options: 
  chunk_output_type: console
draft: false
---

## Housekeeping

```{r echo = F, message= F}
knitr::opts_chunk$set(echo = F, warning = F, messabundance = F)
library(tidyverse)
library(openintro)
library(readr)
library(patchwork)
library(quantreg)
library(kableExtra)
plot_theme <-  theme_minimal() +
  theme(text= element_text(size = 24))
source("../stat201_fns.R")
```

-   Homework 8 due tonight!

-   Project proposal feedback

# Linear regression

Crash course; take STAT 211 for more depth!

## Fitting a line to data

-   Hopefully we are all familiar with the equation of a line: $y = mx + b$

    -   Intercept $b$ and slope $m$ determine specific line

    -   This function is *deterministic*: as long as we know $x$, we know value of $y$ exactly

-   **Simple linear regression**: statistical method where the relationship between variables $x$ and $y$ is modeled as a **line + error:**

::: fragment
$$
y = \underbrace{\beta_{0} \ +\ \beta_{1} x}_{\text{line}} \ + \underbrace{\epsilon}_{\text{error}}
$$
:::

## Simple linear regression model

$$
y = \beta_{0} + \beta_{1} x + \epsilon
$$

-   We have two variables:

    1.  $y$ is response variable. **Must be continuous numerical.**
    2.  $x$ is explanatory variable, also called the **predictor** variable
        -   Can be numerical or categorical

-   $\beta_{0}$ and $\beta_{1}$ are the model **parameters** (intercept and slope)

    -   Estimated using the data, with point estimates $b_{0}$ and $b_{1}$

-   $\epsilon$ (epsilon) represents the **error**

    -   Accounts for variability: we do not expect all data to fall perfectly on the line!

    -   Sometimes we drop the $\epsilon$ term for convenience

## Linear relationship

Suppose we have the following data:

```{r}
set.seed(2)
n <- 50
b0 <- 1
b1 <- 0.5
epsilon <- rnorm(n, 0, 1.5)
x <- sort(runif(n, -5,5))
y <- b0  + b1 * x + epsilon
fit <- coef(lm(y~x))

df <- data.frame(x, y)
p <- ggplot(df, aes(x=x, y = y)) +
  geom_point(size = 3) +
  theme_minimal() +
  theme(text = element_text(size = 24))
p
```

-   Observations won't fall exactly on a line, but do fall around a straight line, so maybe a linear relationship makes sense!

## Fitted values

Suppose we have some specific estimates $b_0$ and $b_{1}$. We could **approximate** the linear relationship using these values as:

$$
\hat{y} = b_{0} + b_{1} x
$$

-   The hat on $y$ signifies an estimate: $\hat{y}$ is the **estimated/fitted** value of $y$ given these specific values of $x$, $b_{0}$ and $b_{1}$

    -   Can obtain a estimate $\hat{y}$ for every observed response $y$

-   Note that the fitted value is obtained *without* the error

## Fitted values (cont.)

```{r}
ids <- c(9, 22, 45)
x_fit <- x[ids]
y_hat <- fit[1] + fit[2] * x
y_fit <- y_hat[ids]
df_fit <- data.frame(x = x_fit, x_end = x_fit, y = y[ids], y_end = y_fit)
p1 <- p + geom_abline(slope = fit[2], intercept = fit[1], col = "yellow", linewidth = 2) +
  labs(title = "Option 1")
p_resid <- p1 + 
  geom_segment(data = df_fit,mapping= aes(x = x, xend = x_end, y = y, yend = y_end), linetype = "dashed", col = "#F8766D")+
    geom_point(data = df_fit, mapping = aes(x = x, y = y_end, col = "fitted"), shape = 4, size = 6)+
  geom_point(data = df_fit, mapping = aes(x = x, y = y, col = "observed"), size = 4) +
  theme(legend.title = element_blank(), plot.title = element_blank()) +
  labs(y = "y", x = "x")
p_resid
```

-   Suppose our estimated line is the yellow one: $\hat{y} = `r round(fit[1],2)` + `r round(fit[2], 2)` x$
-   The fitted value $\hat{y}_{i}$ for $y_{i}$ **lies on the line**; the above plot shows three specific examples

## Residual

**Residuals** (denoted as $e$) are the remaining variation in the data after fitting a model.

$$
\text{observed response} = \text{fit} + \text{residual}
$$

-   For **each** observation $i$, we obtain the residual $e_{i}$ via:

::: fragment
$$y_{i} = \hat{y}_{i} + e_{i} \Rightarrow e_{i} =  y_{i} - \hat{y}_{i}$$
:::

-   Residual = difference between observed and expected

-   In the plot, the residual is indicated by the vertical dashed line

    -   ::: discuss
        What is the ideal value for a residual? What does a positive/negative residual indicate?
        :::

## Residual (cont.)

```{r}
p_resid
```

::: fragment
Residual values for the three highlighted observations:

```{r}
df |>
  add_column(y_hat) |>
  mutate(residual = y - y_hat) |>
  slice(ids) |>
  kable(digits = 3) |>
  kable_styling(font_size = 25)
```
:::

## Residual plot

-   Residuals are very helpful in evaluating how well a model fits a set of data

-   **Residual plot**: original $x$ values plotted against corresponding residuals on $y$-axis

::: fragment
```{r fig.height=5}
df |>
  add_column(y_hat) |>
  mutate(residual = y - y_hat) |>
  ggplot(aes(x = x, y = residual)) +
  geom_point(size = 4) +
  theme_minimal() +
  theme(text = element_text(size = 28)) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_point(data = df_fit, aes(x = x, y = y - y_end), size = 2, col = "#F8766D" ) +
  labs(title = "Residual plot", caption = "Red dots = specific points from previous plot")
```
:::

## Residual plot (cont.)

Residual plots can be useful for identifying characteristics/patterns that remain in the data even after fitting a model.

-   ::: {style="color: maroon"}
    Just because you fit a model to data, does not mean the model is a good fit!
    :::

:::: fragment
![](figs/23-residual-plot.png){fig-align="center" width="506"}

::: discuss
Can you identify any patterns remaining in the residuals?
:::
::::

## Describing linear relationships

Different data may exhibit different strength of linear relationships:

```{r}
set.seed(4)
y2 <- x + rnorm(n, 0, 4)
y3 <- -2*x + rnorm(n,0, 0.25)
y4 <- 1 + 0.01*x + rnorm(n)
data.frame(x=x, y2, y3, y4) |>
  pivot_longer(cols = 2:4, names_to = "Plot", values_to = "y") |>
  mutate(Plot = case_when(
    Plot == "y2" ~ "Data 1",
    Plot == "y3" ~ "Data 2",
    Plot == "y4" ~ "Data 3"
  )) |>
  ggplot(aes(x=x, y =y )) +
  geom_point()+
  facet_wrap(~Plot) +
  theme_minimal() +
  plot_theme
  
```

-   Can we quantify the strength of the linear relationship?

## Correlation

-   **Correlation** is describes the strength of a *linear* relationship between two variables

    -   The observed sample correlation is denoted by $R$
    -   Formula (not important): $R = \frac{1}{n-1} \sum_{i=1}^{n} \left(\frac{x_{i} - \bar{x}}{s_x} \right)\left(\frac{y_{i} - \bar{y}}{s_y} \right)$

:::::: columns
::: {.column width="50%"}
-   Always takes a value between -1 and 1

    -   -1 = perfectly linear and negative

    -   1 = perfectly linear and positive

    -   0 = no linear relationship

-   Nonlinear trends, even when strong, sometimes produce correlations that do not reflect the strength of the relationship
:::

:::: {.column width="50%"}
::: fragment
![](figs/23-correlations.png){fig-align="center" width="1610"}
:::
::::
::::::

# Least squares regression

In Algebra class, there exists a single (intercept, slope) pair because the $(x,y)$ points had no error; all points landed on the line.

-   Now, we assume there is error

-   How do we choose a single "best" $(b_{0}, b_{1})$ pair?

## Different lines

The following display the same set of `r n` observations.

:::::: columns
::: {.column width="70%"}
```{r}
fit_abs <- coef(rq(y ~ x, tau = 0.5))

p2 <- p + geom_abline(slope = fit_abs[2], intercept = fit_abs[1], col = "pink", linewidth = 2) +
  labs(title = "Option 2")
p3 <- p + geom_abline(slope = -1*b1, intercept = b0+0.3, col = "green", linewidth = 2) +
  labs(title = "Option 3")
p1 + p2+ p3
```
:::

:::: {.column width="30%"}
::: discuss
Which line would you say fits the data the best?
:::
::::
::::::

-   There are infinitely many choices of $(b_{0}, b_{1})$ that could be used to create a line

-   We want the BEST choice (i.e. the one that gives us the "line of best fit")

    -   ::: {style="color: maroon"}
        How to define "best"?
        :::

## Line of best fit

One way to define a "best" is to choose the specific values of $(b_{0}, b_{1})$ that minimize the total residuals across all $n$ data points. Results in following possible criterion:

1.  **Least absolute criterion**: minimize sum of residual magnitudes:

::: fragment
$$
|e_{1} | + |e_{2}| + \ldots + |e_{n}|
$$
:::

::: {style="color: maroon"}
2.  **Least squares criterion**: minimize sum of squared residuals:
:::

::: fragment
$$
e_{1}^2 + e_{2}^2 +\ldots + e_{n}^2
$$
:::

-   The choice of $(b_{0}, b_{1})$ that satisfy least squares criterion yields the **least squares line**, and will be our criterion for "best"

-   On previous slide, yellow line is the least squares line, whereas pink line is the least absolute line

## Linear regression model

Remember, our linear regression model is:

$$
y = \beta_{0} + \beta_{1}x + \epsilon
$$

::: fragment
While not wrong, it can be good practice to be specific about an observation $i$:

$$
y_{i} = \beta_{0} + \beta_{1} x_{i} + \epsilon_{i}, \qquad i = 1,\ldots, n
$$
:::

-   Here, we are stating that each observation $i$ has a specific:

    -   explanatory variable value $x_{i}$
    -   response variable value $y_{i}$
    -   error/randomness $\epsilon_{i}$

-   In SLR, we further assume that the errors $\epsilon_{i}$ are independent and Normally distributed

## Conditions for the least squares line (LINE)

Like when using CLT, we should check some conditions before saying a linear regression model is appropriate!

::: {style="color: maroon"}
Assume for now that $x$ is continuous numerical.
:::

1.  **Linearity**: data should show a linear trend between $x$ and $y$

2.  **Independence**: the observations $i$ are independent of each other

    -   e.g. random sample

    -   Non-example: time-series data

3.  **Normality/nearly normal residuals**: the residuals should appear approximately Normal

    -   Possible violations: outliers, influential points (more on this later)

4.  **Equal variability**: variability of points around the least squares line remains roughly constant

## Running example

We will see how to check for these four LINE conditions using the `cherry` data from `openintro`.

::::: columns
::: {.column width="50%"}
```{r}
cherry |>
  select(diam, volume) |>
  slice(1:5)|>
  kable()
```
:::

::: {.column width="50%"}
-   Explanatory variable $x$: `diam`

-   Response variable $y$: `volume`
:::
:::::

::: fragment
Our candidate linear regression model is as follows

$$
\text{volume} = \beta_{0} + \beta_{1} \text{diameter} +\epsilon
$$
:::

## 1. Linearity

Assess *before* fitting the linear regression model by making a scatterplot of $x$ vs. $y$:

```{r}
ggplot(cherry, aes(x= diam, y = volume)) +
  geom_point(size = 3) +
  labs(title = "Cherry tree diameter vs. volume", x = "diameter (inches)", y = "volume (cubic feet)") +
  theme_minimal() +
  theme(text= element_text(size = 24))
```

::: discuss
Does there appear to be a linear relationship between diameter and volume?

-   I would say yes
:::

## 2. Independence

Assess *before* fitting the linear regression model by understanding how your data were sampled.

-   The `cherry` data do not explicitly say that the trees were randomly sampled, but it might be a reasonable assumption

:::::: fragment
An example where independence is violated:

::::: columns
::: {.column width="50%"}
```{r}
set.seed(2)
y <- c(0)
for(i in 2:120){
  if(i < 20 | (45 <= i & i > 60) | (i > 80 & i < 100)){
     to_add <- rnorm(1, y[-i] - 0.2, 0.15)
  } else{
     to_add <- rnorm(1, y[-i] + 0.2, 0.15)
  }
  y <- c(y, to_add)
}
ggplot(data.frame(y) |> mutate(x = row_number()), aes(x = x, y = y)) +
  geom_point(size = 3) +
  plot_theme +
  labs(title = "Time series data")
```
:::

::: {.column width="50%"}
Here, the data are a time series, where observation at time point $i$ depends on the observation at time $i-1$.

-   Successive/consecutive observations are highly correlated
:::
:::::
::::::

## Fitting the model

```{r}
cherry_lm <- lm(volume ~ diam, cherry)
cherry_coefs <- round(coef(cherry_lm), 2)
```

Because the first two conditions are met, we can go ahead and fit the linear regression model (i.e. estimate the values of the coefficients)

-   After fitting the model, we get the following estimates: $b_{0}= `r cherry_coefs[1]`$ and $b_{1} = `r cherry_coefs[2]`$. So our **fitted model** is:

:::: fragment
$$
\widehat{\text{volume}} = `r cherry_coefs[1]` + `r cherry_coefs[2]` \times \text{diameter}
$$

::: {style="color: maroon"}
Remember: the "hat" denotes an estimated/fitted value!
:::
::::

-   We will soon see how $b_{0}$ and $b_{1}$ are calculated and how to interpret them

-   The next two checks can only occur *after* fitting the model.

## 3. Nearly normal residuals

Assess *after* fitting the model by making histogram of residuals and checking for approximate Normality.

-   Remember, residuals are $e_{i} = y_{i} - \hat{y}_{i}$

:::::: fragment
::::: columns
::: {.column width="45%"}
```{r echo = T, eval = F}
cherry |>
  mutate(volume_hat = -36.94 + 5.07*diam) |>
  mutate(residual = volume - volume_hat) 
```

```{r echo = F}
cherry |>
  select(-height) |>
  mutate(volume_hat = cherry_coefs[1] + cherry_coefs[2]*diam) |>
  mutate(residual = volume_hat - volume) |>
  slice(1:5)|>
  kable(digits = 3) |>
  kable_styling(font_size = 20)
```
:::

::: {.column width="55%"}
```{r}
library(broom)
augment(cherry_lm) |>
  ggplot(mapping = aes(x = .resid)) +
  geom_histogram(bins = 20) +
  plot_theme +
  theme(text = element_text(size = 28)) +
  labs(x = "residuals", title="Histogram of residuals for cherry model")
```
:::
:::::
::::::

::: discuss
-   Do the residuals appear approximately Normal?

    -   I think so!
:::

## 4. Equal variance

Assess *after* fitting the model by examining a residual plot and looking for patterns.

::::: columns
::: {.column width="50%"}
A good residual plot:

```{r}
set.seed(15)
x <- seq(1,100, 2)
resid_good <- rnorm(length(x), mean = 0, sd = 4)
data.frame(x, residual = resid_good) |>
  ggplot(aes(x = x, y = residual)) +
  geom_point(size = 4) +
  geom_hline(yintercept = 0, linetype = "dashed", linewidth = 2, col ="darkgrey") +
  plot_theme +
  labs(title = "Constant variance")
```
:::

::: {.column width="50%"}
A bad residual plot:

```{r}
set.seed(12)
resid_bad <- 0 + rnorm(length(x), mean = 0, sd = x/2)
data.frame(x, residual = resid_bad) |>
  ggplot(aes(x = x, y = residual)) +
  geom_point(size = 4) +
  geom_hline(yintercept = 0, linetype = "dashed", linewidth = 2, col ="darkgrey") +
  plot_theme +
  labs(title = "Not constant variance")
```
:::
:::::

We usually add a horizontal line at 0.

## 4. Equal variance (cont.)

Let's examine the residual plot of our fitted model for the `cherry` data:

```{r}
augment(cherry_lm) |>
  ggplot(mapping = aes(x = diam, y = .resid)) +
  geom_point(size = 3) +
  plot_theme +
  geom_hline(yintercept = 0, linetype = "dashed", linewidth =1.25, col ="darkgrey") +
  labs(y = "Residual", title = "Residual plot for cherry model", x = "diameter (inches)")
```

-   ::: discuss
    Do we think equal variance is met?
    :::

    -   I would say there is a definite pattern in the residuals, so equal variance condition is not met.

    -   Some of the variability in the errors appear related to `diameter`
